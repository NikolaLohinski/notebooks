{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theme detection : From Excel to Wikipidia\n",
    "\n",
    "_Author: Nikola LOHINSKI_\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"margin-top: 1em\">\n",
    "In this notebook, you will learn how to :\n",
    "<br>\n",
    "<ul style=\"list-style: none; padding: 0;\">\n",
    "<li>[&#128279;](#1-Parsing-excel-sheets) **Parse an excel sheet** file in python using the open source library **openpyxl** ;</li>\n",
    "<li>[&#128279;](#2-Preprocessing-text-data) **Preprocess text** data using using the open source libraries **nltk** and **pyenchant** ;</li>\n",
    "<li>[&#128279;](#3-Querying-Wikipedia-API) **Querying Wikipedia** API using the open source library **wikitools** in order to retrieve pages, and especially page categories ;</li>\n",
    "<li>[&#128279;](#4-Detecting-themes-in-short-texts) **Detect themes in short texts** contained in excel cells using the 3 previous points.</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "To run this notebook, you will need :\n",
    "<br>\n",
    "<ul>\n",
    "<li>to run on **python 2.7.12** ;</li>\n",
    "<li>to have **openpyxl** installed. Run '**pip install openpyxl**' in the python environment you are using for this notebook ;</li>\n",
    "<li>to have **pyenchant** installed. Run '**pip install pyenchant**' in the python environment you are using for this notebook ;</li>\n",
    "<li>to have **wikitools** installed. Run '**pip install wikitools\n",
    "**' in the python environment you are using for this notebook.</li>\n",
    "<li>to have **nltk** installed. Run '**pip install nltk\n",
    "**' in the python environment you are using for this notebook.</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "Let's start with the imports :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T15:40:56.614371Z",
     "start_time": "2018-03-18T15:40:55.041778Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import enchant\n",
    "import nltk\n",
    "from openpyxl import load_workbook\n",
    "from functools import reduce\n",
    "from wikitools import Wiki\n",
    "from wikitools import APIRequest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to download resource for text data preprocessing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T16:29:23.373411Z",
     "start_time": "2018-03-14T16:29:22.851999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/nikola/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/nikola/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Parsing excel sheets\n",
    "\n",
    "The library **openpyxl** is pretty straightforward The file is loaded and explored the same way it is done when using Google Online Sheet or Microsoft's Excel tool. Let's take a look on an example and display the content of all the cells in a column :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T16:29:23.402830Z",
     "start_time": "2018-03-14T16:29:23.376548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For example lign 0:\n",
      "\n",
      "The crew of a colony ship, bound for a remote planet, discover an uncharted paradise with a threat beyond their imagination, and must attempt a harrowing escape. \n"
     ]
    }
   ],
   "source": [
    "# Load excel file\n",
    "workbook = load_workbook('Fake_EXCEL.xlsx')\n",
    "# Get active worksheet\n",
    "worksheet = workbook.active\n",
    "# Get as specific column\n",
    "column = worksheet['B']\n",
    "texts = list()\n",
    "for i, lign in enumerate(column):\n",
    "    if i > 0:\n",
    "        texts.append(lign.value)\n",
    "print('For example lign 0:\\n\\n{}'.format(texts[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's nice and easy, now let's see how to transform the extracted data to make it neat and clean.\n",
    "\n",
    "# 2 Preprocessing text data\n",
    "\n",
    "In here you will get a sense of how to preprocess text data in order to prepare it for further work. The underlying objective is to clear the sentences and words from anything that is not useful and may endanger whatever comes after in the processing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tokenizing, lowercasing, and filter punctuation\n",
    "First, we need to cast the text a list of lowercased words and remove any left punctuation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T16:29:23.526332Z",
     "start_time": "2018-03-14T16:29:23.406075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For example lign 0 is now:\n",
      "\n",
      "[u'the', u'crew', u'of', u'a', u'colony', u'ship', u'bound', u'for', u'a', u'remote', u'planet', u'discover', u'an', u'uncharted', u'paradise', u'with', u'a', u'threat', u'beyond', u'their', u'imagination', u'and', u'must', u'attempt', u'a', u'harrowing', u'escape']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'[a-zA-Z]+')\n",
    "texts_t = list()\n",
    "for i, text in enumerate(texts):\n",
    "    texts_t.append(tokenizer.tokenize(text.lower()))\n",
    "print('For example lign 0 is now:\\n\\n{}'.format(texts_t[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Removing mispelled words\n",
    "Then, we shall remove words mispelled therefore not present in a dictionary :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T16:29:23.698625Z",
     "start_time": "2018-03-14T16:29:23.529726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For example lign 0 is now:\n",
      "\n",
      "[u'the', u'crew', u'of', u'a', u'colony', u'ship', u'bound', u'for', u'a', u'remote', u'planet', u'discover', u'an', u'uncharted', u'paradise', u'with', u'a', u'threat', u'beyond', u'their', u'imagination', u'and', u'must', u'attempt', u'a', u'harrowing', u'escape']\n"
     ]
    }
   ],
   "source": [
    "dictionary = enchant.Dict('en_US')\n",
    "texts_t_d = list()\n",
    "for i, text in enumerate(texts_t):\n",
    "    texts_t_d.append(list(filter(lambda w: dictionary.check(w), text)))\n",
    "print('For example lign 0 is now:\\n\\n{}'.format(texts_t_d[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "**Note** : to improve this part, when using email that can have a lot of words mispelled, it is possible to use auto-correctors and try correct unknown words, but one has to be carefull regarding correcting contact name for example.\n",
    "</div>\n",
    "\n",
    "### 2.3 Filtering out _stopwords_\n",
    "Finally, we remove any _stopword_ in the sentence. A _stopword_ is a word without any semantic content : 'the', 'a', 'not' etc... It carries semantic information but only to link other words among themselves, and is therefore not a standalone entity : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T16:29:23.802383Z",
     "start_time": "2018-03-14T16:29:23.700807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For example lign 0:\n",
      "\n",
      "[u'crew', u'colony', u'ship', u'bound', u'remote', u'planet', u'discover', u'uncharted', u'paradise', u'threat', u'beyond', u'imagination', u'must', u'attempt', u'harrowing', u'escape']\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "texts_t_d_f = list()\n",
    "for text in texts_t_d:\n",
    "    texts_t_d_f.append(list(filter(lambda w: w not in stopwords, text)))\n",
    "print('For example lign 0:\\n\\n{}'.format(texts_t_d_f[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our text data is now ready for processing. But to be able to query Wikipedia and detect themes, we need to learn how to use its API.\n",
    "\n",
    "# 3 Querying Wikipedia API\n",
    "\n",
    "To query Wikipedia's API, several open source libraries are available. Here we use wikitools because it permits very precise queries to be built, and we need to be able to pass specific search parameters. We will see how to search for a wikipdia page and retrieve its categories.\n",
    "\n",
    "### 3.1 Page search\n",
    "\n",
    "Let's try for example to look for the word `crew` on Wikipedia :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T16:06:22.503885Z",
     "start_time": "2018-03-14T16:06:21.777733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Crew neck', u'J.Crew', u'Film crew', u'Motley crew', u'LBC Crew', u'Crew', u'Cutting Crew', u'Crew (disambiguation)', u'Amanda Crew', u'The Crew']\n",
      "-----------------------------------------------------\n",
      "CPU times: user 4.82 ms, sys: 3.17 ms, total: 7.99 ms\n",
      "Wall time: 718 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "site = Wiki(\"https://en.wikipedia.org/w/api.php\")\n",
    "params = {\n",
    "    \"action\": \"query\",\n",
    "    \"list\": \"search\",\n",
    "    \"srsearch\": 'crew'\n",
    "}\n",
    "\n",
    "request = APIRequest(site, params)\n",
    "r = request.query()\n",
    "result = r['query']['search']\n",
    "\n",
    "titles = {p['pageid']: p['title'] for p in result}\n",
    "\n",
    "print(titles.values())\n",
    "print('-----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieved information from several pages (the title is only printed but other metadata is also available) but the query took some time ($\\sim 0.5 $ seconds on our computer, with the available fast connection). This could be improved by working with an offline version of Wikipedia, or an even better connection to begin with.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "**Important** : it is important to notice that one of the possible listed pages returned by the query is a **disambiguation page**. Those pages are to be handled with caution since they do not have actual useful categories, but do contain semantic links from one them to another.\n",
    "</div>\n",
    "\n",
    "### 3.2 Categories \n",
    "\n",
    "Since we are able to find a page using a search query, we can now create a query to retrieve its categories :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T15:44:42.032107Z",
     "start_time": "2018-03-14T15:44:42.026695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets's use for the example the following page :\n",
      "- title : Crew neck\n",
      "- page id : 28236391\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "example = titles.items()[0]\n",
    "print('Lets\\'s use for the example the following page :')\n",
    "print('- title : {}\\n- page id : {}'.format(example[1], example[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T16:01:12.773036Z",
     "start_time": "2018-03-14T16:01:12.509756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found following categories for page on \"Crew neck\":\n",
      "- Clothing stubs\n",
      "- Necklines\n",
      "- Tops (clothing)\n",
      "--------------------------------------------------\n",
      "CPU times: user 5.97 ms, sys: 0 ns, total: 5.97 ms\n",
      "Wall time: 249 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "example_id = example[0]\n",
    "\n",
    "def result_to_themes(result, page_id):\n",
    "    return [x['title'].replace('Category:', '') for x in result['query']['pages'][str(page_id)]['categories']]\n",
    "\n",
    "params = {\n",
    "  \"action\": \"query\",\n",
    "  \"pageids\": example_id,\n",
    "  \"prop\": \"categories\",\n",
    "  \"clshow\": \"!hidden\"\n",
    "}\n",
    "request = APIRequest(site, params)\n",
    "\n",
    "result = request.query()\n",
    "\n",
    "themes = result_to_themes(result, example_id)\n",
    "print('Found following categories for page on \"{}\":'.format(example[1]))\n",
    "for t in themes:\n",
    "    print('- {}'.format(t))\n",
    "print('--------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the previous point, the request takes some time ($\\sim 0.25$ ms) and this may be of an issue when analysing a sentence.\n",
    "\n",
    "We are now able to get an excel file, extract messages from cells, pre process them into list of useful words, and query Wikipedia with those words : we can start detecting themes in short texts.\n",
    "\n",
    "# 4 Detecting themes in short texts\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "**Important** : the following code cells may take some time to execute depending on your computer performance and Internet connection.\n",
    "</div>\n",
    "\n",
    "The underlying idea is inspired by [Theme Based Clustering of Tweets](http://www.cse.iitd.ernet.in/~bagchi/paper-theme-based.pdf) by Rudra M. Tripathy,Shashank Sharma, Sachindra Joshi, Sameep Mehta and Amitabha Bagchi :\n",
    "- for each meaningful word of a text, we search on Wikipedia for related pages\n",
    "- for each page found, we determine its categories, which correspond to coverted topics in the document\n",
    "- we count occurences of themes and output the most found themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T16:35:23.882775Z",
     "start_time": "2018-03-14T16:34:36.324717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "A game designer on the run from assassins must play her latest virtual reality creation with a marketing trainee to determine if the game has been damaged.\n",
      "\n",
      "Preprocessed text:\n",
      "[u'game', u'designer', u'run', u'assassins', u'must', u'play', u'latest', u'virtual', u'reality', u'creation', u'marketing', u'trainee', u'determine', u'game', u'damaged']\n",
      "\n",
      "Most probable theme occurences:\n",
      "English-language films : 9 times\n",
      "American films : 9 times\n",
      "Action-adventure games : 5 times\n",
      "Assassin's Creed : 5 times\n",
      "Promotion and marketing communications : 4 times\n",
      "PlayStation 4 games : 4 times\n",
      "Directorial debut films : 4 times\n",
      "Reality by type : 3 times\n",
      "Open world video games : 3 times\n",
      "Types of marketing : 3 times\n",
      "Design : 3 times\n",
      "Living people : 3 times\n",
      "2014 video games : 3 times\n",
      "Creation myths : 3 times\n",
      "Marketing techniques : 3 times\n",
      "Digital marketing : 3 times\n",
      "--------------------------------------------------\n",
      "CPU times: user 716 ms, sys: 186 ms, total: 902 ms\n",
      "Wall time: 47.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Example\n",
    "example_id = 2\n",
    "words = texts_t_d_f[example_id]\n",
    "print('Original text:\\n{}\\n'.format(texts[example_id]))\n",
    "print('Preprocessed text:\\n{}\\n'.format(words))\n",
    "candidate_pages = dict()\n",
    "site = Wiki('https://en.wikipedia.org/w/api.php')\n",
    "# Get pages\n",
    "for w in words:\n",
    "    params = { 'action': 'query', 'list': 'search', 'srsearch': w }\n",
    "    request = APIRequest(site, params)\n",
    "    r = request.query()\n",
    "    pages = r['query']['search']\n",
    "    page_ids = [(p['pageid'], p['title']) for p in pages]\n",
    "    for x in page_ids:\n",
    "        page_id, title = x\n",
    "        if candidate_pages.get(title) is None:\n",
    "            candidate_pages[title] = page_id\n",
    "\n",
    "# Get categories\n",
    "site = Wiki(\"https://en.wikipedia.org/w/api.php\")\n",
    "candidate_themes = dict()\n",
    "for p in candidate_pages.values():\n",
    "    params = { \n",
    "        'action': 'query',\n",
    "        'pageids': p,\n",
    "        'prop': 'categories',\n",
    "        'clshow': '!hidden'\n",
    "    }\n",
    "    request = APIRequest(site, params)\n",
    "    result = request.query()\n",
    "    themes = [x['title'].replace('Category:', '') for x in result['query']['pages'][str(p)]['categories']]\n",
    "    if \"Disambiguation pages\" not in themes:\n",
    "        for t in themes:\n",
    "            if candidate_themes.get(t) is None:\n",
    "                candidate_themes[t] = 0\n",
    "            candidate_themes[t] += 1\n",
    "\n",
    "# Get results\n",
    "print('Most probable theme occurences:')\n",
    "for t in sorted(candidate_themes, key=candidate_themes.get, reverse=True):\n",
    "    if candidate_themes[t] < 3:\n",
    "        break\n",
    "    print(t + ' : {} times'.format(candidate_themes[t]))          \n",
    "print('--------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally get a list of themes, ordered by occurence during search phase. Some may not be accurate but in general, with 3 to 4 words, it captures the general underlying theme."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pie-python2",
   "language": "python",
   "name": "pie-python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
