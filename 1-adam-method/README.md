# Stochastic optimization methods : ADAM and more

Authors: 
- [Louis DESCHAMPS](https://github.com/louisdeschamps44)
- [Nikola LOHINSKI](https://github.com/NikolaLohinski)

In this notebook, you will learn :

- About ADAM, a **stochastic optimization method** used to improve backpropagation in neural networks ;
- The **difference** between ADAM and the classic gradient descent
- How to implement a **raw version** of ADAM ;
- That ADAM is one of **several other** optimization methods ;
- How to use those other methods in **Keras**.

To run this notebook, you will need :
- to run on python 3.5.X or greater ;
- to install the requirements : run `pip install -r requirements.txt`
- start the notebook once everything is installed : run `jupyter-notebook 
notebook.ipynb`


